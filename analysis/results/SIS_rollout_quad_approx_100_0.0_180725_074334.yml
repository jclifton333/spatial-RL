results:
  0: {runtime: 68.20107316970825, score: 0.295}
  1: {runtime: 65.35413575172424, score: 0.2988461538461539}
  2: {runtime: 66.8932237625122, score: 0.39076923076923076}
  3: {runtime: 68.04367280006409, score: 0.35115384615384615}
  4: {runtime: 66.65524482727051, score: 0.39076923076923076}
  5: {runtime: 67.78432250022888, score: 0.3861538461538462}
  6: {runtime: 67.74098110198975, score: 0.20076923076923076}
  7: {runtime: 68.24890470504761, score: 0.3553846153846154}
  8: {runtime: 68.68103814125061, score: 0.3619230769230769}
  9: {runtime: 67.84662771224976, score: 0.3269230769230769}
  10: {runtime: 65.21678590774536, score: 0.37538461538461537}
  11: {runtime: 66.15862703323364, score: 0.45076923076923076}
  12: {runtime: 68.18577122688293, score: 0.3123076923076923}
  13: {runtime: 65.92085027694702, score: 0.455}
  14: {runtime: 67.53928852081299, score: 0.3646153846153846}
  15: {runtime: 66.28192043304443, score: 0.4065384615384615}
  16: {runtime: 68.3793351650238, score: 0.39807692307692305}
  17: {runtime: 67.86152052879333, score: 0.35346153846153844}
  18: {runtime: 66.76001334190369, score: 0.3688461538461538}
  19: {runtime: 67.59270119667053, score: 0.4}
  20: {runtime: 67.00972008705139, score: 0.36923076923076925}
  21: {runtime: 67.36096715927124, score: 0.3523076923076923}
  22: {runtime: 68.18350625038147, score: 0.37115384615384617}
  23: {runtime: 67.93854093551636, score: 0.3476923076923077}
  24: {runtime: 67.42871928215027, score: 0.46307692307692305}
  25: {runtime: 67.03469204902649, score: 0.435}
  26: {runtime: 66.9043083190918, score: 0.31384615384615383}
  27: {runtime: 67.71225261688232, score: 0.31153846153846154}
  28: {runtime: 67.6975998878479, score: 0.3496153846153846}
  29: {runtime: 67.77229070663452, score: 0.3696153846153846}
  30: {runtime: 68.08754372596741, score: 0.36230769230769233}
  31: {runtime: 69.23773431777954, score: 0.3553846153846154}
  32: {runtime: 67.4429829120636, score: 0.30615384615384617}
  33: {runtime: 69.66503953933716, score: 0.3830769230769231}
  34: {runtime: 69.43258380889893, score: 0.3638461538461538}
  35: {runtime: 68.63348960876465, score: 0.4076923076923077}
  36: {runtime: 68.0018904209137, score: 0.29653846153846153}
  37: {runtime: 67.58412861824036, score: 0.38576923076923075}
  38: {runtime: 68.6585168838501, score: 0.36}
  39: {runtime: 67.90311551094055, score: 0.3769230769230769}
  40: {runtime: 68.81735348701477, score: 0.36846153846153845}
  41: {runtime: 67.20481705665588, score: 0.32576923076923076}
  42: {runtime: 69.28217697143555, score: 0.38153846153846155}
  43: {runtime: 68.58603239059448, score: 0.43423076923076925}
  44: {runtime: 68.61798477172852, score: 0.38961538461538464}
  45: {runtime: 67.50160884857178, score: 0.35615384615384615}
  46: {runtime: 67.79276442527771, score: 0.235}
  47: {runtime: 68.29564714431763, score: 0.32269230769230767}
  48: {runtime: 28.452051877975464, score: 0.32}
  49: {runtime: 28.21770191192627, score: 0.33807692307692305}
  mean: 0.3599
  se: !!python/object/apply:numpy.core.multiarray.scalar
  - !!python/object/apply:numpy.dtype
    args: [f8, 0, 1]
    state: !!python/tuple [3, <, null, null, null, -1, -1, 0]
  - !!binary |
    ew4MiESOfD8=
settings: {L: 100, argmaxer: quad_approx, argmaxer_name: quad_approx, classifier: KerasLogit,
  divide_evenly: false, env_name: sis, evaluation_budget: 100, gamma: 0.9, number_of_replicates: 50,
  planning_depth: 25, policy_name: rollout, regressor: RandomForestRegressor, rollout_depth: 0,
  time_horizon: 25, treatment_budget: 5}
