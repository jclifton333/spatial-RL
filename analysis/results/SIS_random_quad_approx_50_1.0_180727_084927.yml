results:
  0: {runtime: 0.22183871269226074, score: 0.3853846153846154}
  1: {runtime: 0.26793813705444336, score: 0.4169230769230769}
  2: {runtime: 0.2719247341156006, score: 0.4276923076923077}
  3: {runtime: 0.26176881790161133, score: 0.35846153846153844}
  4: {runtime: 0.2681877613067627, score: 0.3046153846153846}
  5: {runtime: 0.21197175979614258, score: 0.2923076923076923}
  6: {runtime: 0.18816328048706055, score: 0.43538461538461537}
  7: {runtime: 0.19754290580749512, score: 0.45076923076923076}
  8: {runtime: 0.2101278305053711, score: 0.3953846153846154}
  9: {runtime: 0.2057051658630371, score: 0.39692307692307693}
  10: {runtime: 0.20842242240905762, score: 0.42615384615384616}
  11: {runtime: 0.19068193435668945, score: 0.36230769230769233}
  12: {runtime: 0.18663287162780762, score: 0.4584615384615385}
  13: {runtime: 0.20417046546936035, score: 0.3}
  14: {runtime: 0.21083760261535645, score: 0.3776923076923077}
  15: {runtime: 0.19027066230773926, score: 0.4423076923076923}
  16: {runtime: 0.20264983177185059, score: 0.3707692307692308}
  17: {runtime: 0.20317482948303223, score: 0.40692307692307694}
  18: {runtime: 0.19426202774047852, score: 0.4523076923076923}
  19: {runtime: 0.1360464096069336, score: 0.39}
  20: {runtime: 0.18986773490905762, score: 0.37615384615384617}
  21: {runtime: 0.1860191822052002, score: 0.4715384615384615}
  22: {runtime: 0.21773791313171387, score: 0.4176923076923077}
  23: {runtime: 0.18793296813964844, score: 0.43153846153846154}
  24: {runtime: 0.19332504272460938, score: 0.35384615384615387}
  25: {runtime: 0.19053077697753906, score: 0.3546153846153846}
  26: {runtime: 0.20552301406860352, score: 0.14923076923076922}
  27: {runtime: 0.22451519966125488, score: 0.4307692307692308}
  28: {runtime: 0.1936054229736328, score: 0.45153846153846156}
  29: {runtime: 0.18705511093139648, score: 0.40923076923076923}
  30: {runtime: 0.18442845344543457, score: 0.44153846153846155}
  31: {runtime: 0.19289827346801758, score: 0.36615384615384616}
  32: {runtime: 0.18780827522277832, score: 0.43923076923076926}
  33: {runtime: 0.18596315383911133, score: 0.3723076923076923}
  34: {runtime: 0.18599891662597656, score: 0.45692307692307693}
  35: {runtime: 0.1857318878173828, score: 0.4207692307692308}
  36: {runtime: 0.19104433059692383, score: 0.39153846153846156}
  37: {runtime: 0.19942021369934082, score: 0.4246153846153846}
  38: {runtime: 0.19008398056030273, score: 0.34923076923076923}
  39: {runtime: 0.1809389591217041, score: 0.4776923076923077}
  40: {runtime: 0.2234058380126953, score: 0.3123076923076923}
  41: {runtime: 0.16506028175354004, score: 0.42846153846153845}
  42: {runtime: 0.18437886238098145, score: 0.4776923076923077}
  43: {runtime: 0.17504549026489258, score: 0.4430769230769231}
  44: {runtime: 0.21492266654968262, score: 0.34}
  45: {runtime: 0.1566448211669922, score: 0.42538461538461536}
  46: {runtime: 0.19221138954162598, score: 0.3030769230769231}
  47: {runtime: 0.1876223087310791, score: 0.41615384615384615}
  48: {runtime: 0.08937859535217285, score: 0.2230769230769231}
  49: {runtime: 0.11633634567260742, score: 0.39384615384615385}
  mean: 0.392
  se: !!python/object/apply:numpy.core.multiarray.scalar
  - !!python/object/apply:numpy.dtype
    args: [f8, 0, 1]
    state: !!python/tuple [3, <, null, null, null, -1, -1, 0]
  - !!binary |
    60po41p5gj8=
settings: {L: 50, argmaxer: quad_approx, argmaxer_name: quad_approx, classifier: KerasLogit,
  divide_evenly: false, env_name: SIS, evaluation_budget: 100, gamma: 0.9, number_of_replicates: 50,
  planning_depth: 25, policy_name: random, regressor: KerasRegressor, rollout_depth: 1,
  time_horizon: 25, treatment_budget: 3}
